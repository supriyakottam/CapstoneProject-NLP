{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMEG8mPY/QJrMn2/H2sfPyM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"teds_ILBkdgZ","executionInfo":{"status":"ok","timestamp":1718503159793,"user_tz":240,"elapsed":13865,"user":{"displayName":"Supriya","userId":"02451596946450165236"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8540188a-54b8-4822-f2a4-504174e32aa6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}],"source":["import zipfile\n","import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pad_sequence\n","import time\n","import re\n","import string\n","import nltk\n","from nltk.corpus import stopwords\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator"]},{"cell_type":"code","source":[],"metadata":{"id":"YbMdImg2lVpd","executionInfo":{"status":"ok","timestamp":1718503159794,"user_tz":240,"elapsed":13,"user":{"displayName":"Supriya","userId":"02451596946450165236"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["zip_file = 'comments2k.zip'   # Adding our zip file  to extract the information\n","\n","with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n","    zip_ref.extractall('/content')\n"],"metadata":{"id":"MFXDwydCku8G","executionInfo":{"status":"ok","timestamp":1718503161031,"user_tz":240,"elapsed":1245,"user":{"displayName":"Supriya","userId":"02451596946450165236"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D2xcKlurDO2Y","executionInfo":{"status":"ok","timestamp":1718503161031,"user_tz":240,"elapsed":11,"user":{"displayName":"Supriya","userId":"02451596946450165236"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","# Downloading stopwords like “the”, “a”, “an”, “so”, “what”.\n","#nltk.download('stopwords') #downloading them from Natural Language Toolkit\n","#stop_words = set(stopwords.words('english'))\n","\n","#def clean_text(text):\n","    # Convert text to lowercase\n","    #text = text.lower()\n","    # Remove URLs\n","    # Remove punctuation\n","    #text = text.translate(str.maketrans('', '', string.punctuation))\n","    # Remove numbers\n","    #text = re.sub(r'\\d+', '', text)\n","    # Remove stopwords\n","    #text = ' '.join(word for word in text.split() if word not in stop_words)\n","    # Remove extra whitespaces\n","    #text = ' '.join(text.split())\n","    #return text\n","\n","def load_comments(dir_path):\n","    comments = []\n","    for filename in os.listdir(dir_path):\n","        file_path = os.path.join(dir_path, filename)\n","        if os.path.isfile(file_path):\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                comments.extend(f.readlines())\n","    return comments\n","\n","pos_dir = '/content/comments1k_pos'\n","neg_dir = '/content/comments1k_neg'\n","\n","positive_comments = load_comments(pos_dir)\n","negative_comments = load_comments(neg_dir)\n","\n","#positive_comments = [clean_text(comment) for comment in positive_comments]\n","#negative_comments = [clean_text(comment) for comment in negative_comments]\n","\n","texts = positive_comments + negative_comments\n","labels = ['Positive'] * len(positive_comments) + ['Negative'] * len(negative_comments)\n","\n","data = pd.DataFrame({'text': texts, 'sentiment': labels})\n"],"metadata":{"id":"2XgPxQQxqhf-","executionInfo":{"status":"ok","timestamp":1718503161270,"user_tz":240,"elapsed":245,"user":{"displayName":"Supriya","userId":"02451596946450165236"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"1KoOG_grqs-y","executionInfo":{"status":"ok","timestamp":1718503161490,"user_tz":240,"elapsed":225,"user":{"displayName":"Supriya","userId":"02451596946450165236"}},"outputId":"a59d2c53-dd24-478f-9a01-188bd377635c"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   text sentiment\n","0     This movie is so misunderstood it is not even ...  Positive\n","1     Okay, first of all I got this movie as a Chris...  Positive\n","2     The performances were superb, the costumes del...  Positive\n","3     I've been trying to find out about this series...  Positive\n","4     yeah right. Sammo Hung already acted in the ma...  Positive\n","...                                                 ...       ...\n","1995  It occurs to me that some of the films that ha...  Negative\n","1996  Firstly, there are some good things about this...  Negative\n","1997  Garson Kanin wrote and directed this look at \"...  Negative\n","1998  This movie is one of the worst movies I have e...  Negative\n","1999  God, I was bored out of my head as I watched t...  Negative\n","\n","[2000 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-95c5ded9-0f3c-4654-bb9b-13854da31d31\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>This movie is so misunderstood it is not even ...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Okay, first of all I got this movie as a Chris...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The performances were superb, the costumes del...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I've been trying to find out about this series...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>yeah right. Sammo Hung already acted in the ma...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>It occurs to me that some of the films that ha...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>Firstly, there are some good things about this...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>Garson Kanin wrote and directed this look at \"...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>This movie is one of the worst movies I have e...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>God, I was bored out of my head as I watched t...</td>\n","      <td>Negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95c5ded9-0f3c-4654-bb9b-13854da31d31')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-95c5ded9-0f3c-4654-bb9b-13854da31d31 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-95c5ded9-0f3c-4654-bb9b-13854da31d31');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-84c4a1eb-3a4a-4188-981b-240198da5eb3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84c4a1eb-3a4a-4188-981b-240198da5eb3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-84c4a1eb-3a4a-4188-981b-240198da5eb3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_590021ea-3599-4762-8280-fafef285a610\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_590021ea-3599-4762-8280-fafef285a610 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('data');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 2000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1994,\n        \"samples\": [\n          \"The film is hugely enjoyable with a great cast, and excellent direction by James Eves. The movie is entertaining with a very charismatic performance from Stephanie Beecham and everyone is perfectly cast. James Eves has a good eye for casting and directs like a conductor knowing exactly when to crank up the action, fall and then rise to a climax. He does this with an element of humour, Plenty of twists, thrills and blood. This is a return of the old vampire movie, with loads of gore, blood and screams. The movie works at a great speed and the characters take you on a terrific adventure,but what makes it work is that the film doesn't take itself too seriously with plenty of tongue in cheek action.Great !\",\n          \"Jill Dunne (played by Mitzi Kapture), is an attractive, nice woman, over-whelmed by a smart-mouthed teenage daughter, Liv (Martha MacIsaac) and a petty, two-timing husband, Sean (Rick Roberts), both of which were tediously self-centered, and obnoxious.<br /><br />This was advertised as a troubled family stalked by a crazed killer during a relentless storm.<br /><br />The storm doesn't even happen until about the last 5 minutes of the film, and then it isn't anything to send anybody running to the storm cellar.<br /><br />The stalking, likewise doesn't get intense until almost the end of the film.<br /><br />Most of the film we spend listening to Jill and her insufferable daughter, Liv, argue until I just wanted to back slap the daughter into next week.<br /><br />Jill's problem with Liv is that she has taken up with Zack, a boy of questionable character, and they are constantly making out--in fact Jill comes home to find the two of them on Liv's bed.<br /><br />The rest of the time we spend listening to Jill's husband Sean either whine at Jill or criticize her.<br /><br />Sean was not at all appealing--since his face is so covered in freckles you could play connect the dots.<br /><br />The story begins with Jill being notified of an out-standing bill on their credit card for a hotel she has never been to, and that she thought Sean had never been to either.<br /><br />Jill goes to the hotel where she meets the owner & manager, Richard Grant (Nick Mancuso), a very nice, older, divorced man, who is sympathetic to her. In fact, when he spots her husband there again, he phones Jill and tips her off.<br /><br />Jill returns to the hotel, sees Sean with another woman. She is upset, leaves without Sean seeing her, and does absolutely nothing. In fact, she doesn't even say anything to Sean when he arrives home. This made no sense to me.<br /><br />Jill has given Richard her business card, and so he calls her and she is apparently in real estate. She shows him a condo. Afterwards they have a drink, and things get cozy between them.<br /><br />Richard and Jill are getting it on, hot and heavy. In fact, he seems a bit more aggressive than necessary, when Jill suddenly decides to cut out.<br /><br />Jill and Sean have a confrontation about his cheating. Sean whines about how Jill has been letting him down since her father died. Apparently his lack of any morals is all her fault. Eventually Jill confesses her own lack of morals and near adultery to Sean--and of course that's all her fault too, as far as Sean is concerned.<br /><br />The little family decides to go on a camping trip--which means more whining and grousing among them, especially from the spoiled daughter.<br /><br />I was so rooting for the stalker to get everybody, but Jill.<br /><br />3 stars\",\n          \"It starts slowly, showing the dreary lives of the two housewives who decide to rent a castle in Italy for the month of April, but don't give up on it. Nothing much happens, but the time passes exquisitely, and there are numerous sly jokes (my favorite is the carriage ride in the storm, which I find hilarious). The movie is wonderfully romantic in many senses of the word, the scenery is beautiful (as is Polly Walker), and the resolutions in the movie are very satisfying.<br /><br />The movie takes a couple of liberties with the book, the biggest being with the Arbuthnot/Briggs/Dester business, but I actually preferred the movie's version of this (it may be more sentimental, but I felt that it was more consistent with the tone of the story, and anyway I like sentiment when it's well done).<br /><br />An excellent movie, especially as a date movie during lousy weather.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Negative\",\n          \"Positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["\n"],"metadata":{"id":"7m-ZFy0Uk4Te","executionInfo":{"status":"ok","timestamp":1718503161491,"user_tz":240,"elapsed":10,"user":{"displayName":"Supriya","userId":"02451596946450165236"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n","\n","# Define and fit the tokenizer\n","tokenizer = get_tokenizer('basic_english')\n","\n","def yield_tokens(data_iter):\n","    for text in data_iter:\n","        yield tokenizer(text)\n","\n","vocab = build_vocab_from_iterator(yield_tokens(train_texts), specials=[\"<unk>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])\n","\n","# Tokenize and convert texts to sequences\n","train_sequences = [torch.tensor([vocab[token] for token in tokenizer(text.lower())]) for text in train_texts]\n","val_sequences = [torch.tensor([vocab[token] for token in tokenizer(text.lower())]) for text in val_texts]\n","test_sequences = [torch.tensor([vocab[token] for token in tokenizer(text.lower())]) for text in test_texts]\n","\n","# Pad the sequences\n","def pad_sequences(sequences, max_len):\n","    return pad_sequence(sequences, batch_first=True, padding_value=0)\n","\n","max_len = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences), max(len(seq) for seq in test_sequences))\n","\n","train_padded = pad_sequences(train_sequences, max_len)\n","val_padded = pad_sequences(val_sequences, max_len)\n","test_padded = pad_sequences(test_sequences, max_len)\n","\n","# Define the vocabulary size\n","vocab_size = len(vocab)+1\n","\n","print(\"Vocabulary size:\", vocab_size)\n","print(\"Train padded shape:\", train_padded.shape)\n","print(\"Validation padded shape:\", val_padded.shape)\n","print(\"Test padded shape:\", test_padded.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IlvaU5m0t2bv","executionInfo":{"status":"ok","timestamp":1718503162922,"user_tz":240,"elapsed":1440,"user":{"displayName":"Supriya","userId":"02451596946450165236"}},"outputId":"7af97a73-ab86-4743-d6a6-5b1e2a24538c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 20745\n","Train padded shape: torch.Size([1400, 1212])\n","Validation padded shape: torch.Size([300, 1149])\n","Test padded shape: torch.Size([300, 1469])\n"]}]},{"cell_type":"code","source":["# Encode the labels\n","label_encoder = LabelEncoder()\n","train_labels_encoded = label_encoder.fit_transform(train_labels)\n","val_labels_encoded = label_encoder.transform(val_labels)\n","test_labels_encoded = label_encoder.transform(test_labels)\n","\n","# Convert labels to torch tensors\n","train_labels_tensor = torch.tensor(train_labels_encoded, dtype=torch.long)\n","val_labels_tensor = torch.tensor(val_labels_encoded, dtype=torch.long)\n","test_labels_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n","\n","# Create TensorDataset\n","train_data = TensorDataset(train_padded, train_labels_tensor)\n","val_data = TensorDataset(val_padded, val_labels_tensor)\n","test_data = TensorDataset(test_padded, test_labels_tensor)\n","\n","# Create DataLoader\n","train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n","\n","# Hyperparameters\n","embedding_dim = 200  # Dimension of word embeddings\n","hidden_size = 256\n","num_classes = 2\n","learning_rate = 0.001\n","num_epochs = 20"],"metadata":{"id":"QqbVmhFvkn2a","executionInfo":{"status":"ok","timestamp":1718503162922,"user_tz":240,"elapsed":18,"user":{"displayName":"Supriya","userId":"02451596946450165236"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kTleVr4FBhiV","executionInfo":{"status":"ok","timestamp":1718503163152,"user_tz":240,"elapsed":10,"user":{"displayName":"Supriya","userId":"02451596946450165236"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Simple Neural Network Model\n","class SimpleNNModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n","        super(SimpleNNModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.fc1 = nn.Linear(embedding_dim, hidden_size)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        embedded = embedded.mean(dim=1)  # Average embeddings over the sequence\n","        out = self.fc1(embedded)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out\n","\n","# RNN Model\n","class RNNModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n","        super(RNNModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        h0 = torch.zeros(1, x.size(0), self.rnn.hidden_size).to(x.device)\n","        out, _ = self.rnn(embedded, h0)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n","        super(LSTMModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        h0 = torch.zeros(1, x.size(0), self.lstm.hidden_size).to(x.device)\n","        c0 = torch.zeros(1, x.size(0), self.lstm.hidden_size).to(x.device)\n","        out, _ = self.lstm(embedded, (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# GRU Model\n","class GRUModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n","        super(GRUModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        h0 = torch.zeros(1, x.size(0), self.gru.hidden_size).to(x.device)\n","        out, _ = self.gru(embedded, h0)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","class BiLSTM3Layers(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes, dropout=0.2):\n","        super(BiLSTM3Layers, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=3, batch_first=True, dropout=dropout, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Multiply by 2 for bidirectional LSTM\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        # Concatenate the final hidden states from both directions\n","        lstm_out = torch.cat((lstm_out[:, -1, :hidden_size], lstm_out[:, 0, hidden_size:]), dim=1)\n","        output = self.fc(lstm_out)\n","        return output\n","\n"],"metadata":{"id":"OPO333pNlWgr","executionInfo":{"status":"ok","timestamp":1718503163152,"user_tz":240,"elapsed":9,"user":{"displayName":"Supriya","userId":"02451596946450165236"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","\n","def train_and_evaluate(model, train_loader, val_loader, test_loader, num_epochs, learning_rate):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        train_correct = 0\n","        train_total = 0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            train_total += labels.size(0)\n","            train_correct += (predicted == labels).sum().item()\n","        train_losses.append(train_loss / len(train_loader.dataset))\n","        train_accuracy = train_correct / train_total\n","        train_accuracies.append(train_accuracy)\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * inputs.size(0)\n","                _, predicted = torch.max(outputs.data, 1)\n","                val_total += labels.size(0)\n","                val_correct += (predicted == labels).sum().item()\n","\n","        val_losses.append(val_loss / len(val_loader.dataset))\n","        val_accuracy = val_correct / val_total\n","        val_accuracies.append(val_accuracy)\n","\n","        print(f'Model: {type(model).__name__}, Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracy:.4f}')\n","\n","    time_cost = time.time() - start_time\n","\n","    # Further evaluation on validation and test sets\n","    model.eval()\n","    val_preds = []\n","    val_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            val_preds.extend(predicted.cpu().numpy())\n","            val_labels.extend(labels.cpu().numpy())\n","    val_report = classification_report(val_labels, val_preds, target_names=['Negative', 'Positive'])\n","\n","    test_preds = []\n","    test_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            test_preds.extend(predicted.cpu().numpy())\n","            test_labels.extend(labels.cpu().numpy())\n","    test_report = classification_report(test_labels, test_preds, target_names=['Negative', 'Positive'])\n","\n","\n","    return train_losses, val_losses, train_accuracies, val_accuracies, val_report, test_report, time_cost\n","\n","\n","simple_nn = SimpleNNModel(vocab_size, embedding_dim, hidden_size, num_classes)\n","simple_nn_train_losses, simple_nn_val_losses, simple_nn_train_accuracies, simple_nn_val_accuracies, simple_nn_val_report, simple_nn_test_report, simple_nn_time_cost = train_and_evaluate(simple_nn, train_loader, val_loader, test_loader, num_epochs, learning_rate)\n","\n","print(\"Simple NN Classification Report (Validation):\")\n","print(simple_nn_val_report)\n","print(\"Simple NN Classification Report (Test):\")\n","print(simple_nn_test_report)\n","print(f\"Simple NN Time Cost: {simple_nn_time_cost:.2f} seconds\")\n","print(\"=\"*60)\n","# Train and evaluate RNN\n","rnn = RNNModel(vocab_size, embedding_dim, hidden_size, num_classes)\n","rnn_train_losses, rnn_val_losses, rnn_train_accuracies, rnn_val_accuracies, rnn_val_report, rnn_test_report, rnn_time_cost = train_and_evaluate(rnn, train_loader, val_loader, test_loader, num_epochs, learning_rate)\n","\n","print(\"RNN Classification Report (Validation):\")\n","print(rnn_val_report)\n","print(\"RNN Classification Report (Test):\")\n","print(rnn_test_report)\n","print(f\"RNN Time Cost: {rnn_time_cost:.2f} seconds\")\n","print(\"=\"*60)\n","# Train and evaluate LSTM\n","lstm = LSTMModel(vocab_size, embedding_dim, hidden_size, num_classes)\n","lstm_train_losses, lstm_val_losses, lstm_train_accuracies, lstm_val_accuracies, lstm_val_report, lstm_test_report, lstm_time_cost = train_and_evaluate(lstm, train_loader, val_loader, test_loader, num_epochs, learning_rate)\n","\n","print(\"LSTM Classification Report (Validation):\")\n","print(lstm_val_report)\n","print(\"LSTM Classification Report (Test):\")\n","print(lstm_test_report)\n","print(f\"LSTM Time Cost: {lstm_time_cost:.2f} seconds\")\n","print(\"=\"*60)\n","# Train and evaluate GRU\n","gru = GRUModel(vocab_size, embedding_dim, hidden_size, num_classes)\n","gru_train_losses, gru_val_losses, gru_train_accuracies, gru_val_accuracies, gru_val_report, gru_test_report, gru_time_cost = train_and_evaluate(gru, train_loader, val_loader, test_loader, num_epochs, learning_rate)\n","\n","print(\"GRU Classification Report (Validation):\")\n","print(gru_val_report)\n","print(\"GRU Classification Report (Test):\")\n","print(gru_test_report)\n","print(f\"GRU Time Cost: {gru_time_cost:.2f} seconds\")\n","print(\"=\"*60)\n","biLSTM = BiLSTM3Layers(vocab_size, embedding_dim, hidden_size, num_classes,dropout=0.2)\n","biLSTM_train_losses, biLSTM_val_losses, biLSTM_train_accuracies, biLSTM_val_accuracies, biLSTM_val_report, biLSTM_test_report, biLSTM_time_cost = train_and_evaluate(biLSTM, train_loader, val_loader, test_loader, num_epochs, learning_rate)\n","\n","print(\"biLSTM Classification Report (Validation):\")\n","print(biLSTM_val_report)\n","print(\"biLSTM Classification Report (Test):\")\n","print(biLSTM_test_report)\n","print(f\"biLSTM Time Cost: {biLSTM_time_cost:.2f} seconds\")\n","\n"],"metadata":{"id":"JlJBoIS_veQt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6bbecc38-5c0a-426f-cf76-5a5cba9e9906"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: SimpleNNModel, Epoch 1/20, Train Loss: 0.7107, Train Acc: 0.4986, Val Loss: 0.6970, Val Acc: 0.5000\n","Model: SimpleNNModel, Epoch 2/20, Train Loss: 0.6956, Train Acc: 0.5193, Val Loss: 0.6963, Val Acc: 0.4933\n","Model: SimpleNNModel, Epoch 3/20, Train Loss: 0.6992, Train Acc: 0.5086, Val Loss: 0.6921, Val Acc: 0.5033\n","Model: SimpleNNModel, Epoch 4/20, Train Loss: 0.6939, Train Acc: 0.5271, Val Loss: 0.6952, Val Acc: 0.5033\n","Model: SimpleNNModel, Epoch 5/20, Train Loss: 0.6914, Train Acc: 0.5257, Val Loss: 0.6926, Val Acc: 0.4900\n","Model: SimpleNNModel, Epoch 6/20, Train Loss: 0.6914, Train Acc: 0.5279, Val Loss: 0.6982, Val Acc: 0.4900\n","Model: SimpleNNModel, Epoch 7/20, Train Loss: 0.6925, Train Acc: 0.5057, Val Loss: 0.6902, Val Acc: 0.5267\n","Model: SimpleNNModel, Epoch 8/20, Train Loss: 0.6865, Train Acc: 0.5429, Val Loss: 0.6984, Val Acc: 0.4900\n","Model: SimpleNNModel, Epoch 9/20, Train Loss: 0.6811, Train Acc: 0.5421, Val Loss: 0.6861, Val Acc: 0.5000\n","Model: SimpleNNModel, Epoch 10/20, Train Loss: 0.6753, Train Acc: 0.6129, Val Loss: 0.6885, Val Acc: 0.5167\n","Model: SimpleNNModel, Epoch 11/20, Train Loss: 0.6776, Train Acc: 0.5636, Val Loss: 0.6799, Val Acc: 0.5233\n","Model: SimpleNNModel, Epoch 12/20, Train Loss: 0.6699, Train Acc: 0.5786, Val Loss: 0.6997, Val Acc: 0.5100\n","Model: SimpleNNModel, Epoch 13/20, Train Loss: 0.6590, Train Acc: 0.5907, Val Loss: 0.6647, Val Acc: 0.5800\n","Model: SimpleNNModel, Epoch 14/20, Train Loss: 0.6423, Train Acc: 0.6736, Val Loss: 0.6532, Val Acc: 0.5933\n","Model: SimpleNNModel, Epoch 15/20, Train Loss: 0.6219, Train Acc: 0.6750, Val Loss: 0.6502, Val Acc: 0.5433\n","Model: SimpleNNModel, Epoch 16/20, Train Loss: 0.5825, Train Acc: 0.7729, Val Loss: 0.6008, Val Acc: 0.7900\n","Model: SimpleNNModel, Epoch 17/20, Train Loss: 0.5417, Train Acc: 0.7793, Val Loss: 0.6477, Val Acc: 0.5600\n","Model: SimpleNNModel, Epoch 18/20, Train Loss: 0.5471, Train Acc: 0.7021, Val Loss: 0.5398, Val Acc: 0.8000\n","Model: SimpleNNModel, Epoch 19/20, Train Loss: 0.4499, Train Acc: 0.8486, Val Loss: 0.5125, Val Acc: 0.7967\n","Model: SimpleNNModel, Epoch 20/20, Train Loss: 0.4053, Train Acc: 0.8857, Val Loss: 0.4829, Val Acc: 0.8000\n","Simple NN Classification Report (Validation):\n","              precision    recall  f1-score   support\n","\n","    Negative       0.86      0.71      0.78       147\n","    Positive       0.76      0.89      0.82       153\n","\n","    accuracy                           0.80       300\n","   macro avg       0.81      0.80      0.80       300\n","weighted avg       0.81      0.80      0.80       300\n","\n","Simple NN Classification Report (Test):\n","              precision    recall  f1-score   support\n","\n","    Negative       0.87      0.75      0.80       151\n","    Positive       0.78      0.89      0.83       149\n","\n","    accuracy                           0.82       300\n","   macro avg       0.82      0.82      0.82       300\n","weighted avg       0.82      0.82      0.82       300\n","\n","Simple NN Time Cost: 80.05 seconds\n","============================================================\n","Model: RNNModel, Epoch 1/20, Train Loss: 0.7724, Train Acc: 0.5007, Val Loss: 0.7322, Val Acc: 0.4900\n","Model: RNNModel, Epoch 2/20, Train Loss: 0.7002, Train Acc: 0.4964, Val Loss: 0.6924, Val Acc: 0.5133\n","Model: RNNModel, Epoch 3/20, Train Loss: 0.7005, Train Acc: 0.5000, Val Loss: 0.7066, Val Acc: 0.4900\n","Model: RNNModel, Epoch 4/20, Train Loss: 0.7032, Train Acc: 0.5086, Val Loss: 0.6950, Val Acc: 0.4900\n","Model: RNNModel, Epoch 5/20, Train Loss: 0.6952, Train Acc: 0.5014, Val Loss: 0.6919, Val Acc: 0.4900\n","Model: RNNModel, Epoch 6/20, Train Loss: 0.6961, Train Acc: 0.5050, Val Loss: 0.6922, Val Acc: 0.5133\n","Model: RNNModel, Epoch 7/20, Train Loss: 0.6937, Train Acc: 0.5100, Val Loss: 0.6917, Val Acc: 0.4900\n","Model: RNNModel, Epoch 8/20, Train Loss: 0.7067, Train Acc: 0.5050, Val Loss: 0.7102, Val Acc: 0.5133\n","Model: RNNModel, Epoch 9/20, Train Loss: 0.7026, Train Acc: 0.4779, Val Loss: 0.6913, Val Acc: 0.5133\n","Model: RNNModel, Epoch 10/20, Train Loss: 0.6982, Train Acc: 0.4907, Val Loss: 0.6942, Val Acc: 0.5133\n","Model: RNNModel, Epoch 11/20, Train Loss: 0.6991, Train Acc: 0.4857, Val Loss: 0.6958, Val Acc: 0.4900\n","Model: RNNModel, Epoch 12/20, Train Loss: 0.6942, Train Acc: 0.5000, Val Loss: 0.6916, Val Acc: 0.4933\n","Model: RNNModel, Epoch 13/20, Train Loss: 0.6943, Train Acc: 0.4900, Val Loss: 0.6923, Val Acc: 0.4933\n","Model: RNNModel, Epoch 14/20, Train Loss: 0.6981, Train Acc: 0.5086, Val Loss: 0.6946, Val Acc: 0.4900\n","Model: RNNModel, Epoch 15/20, Train Loss: 0.6943, Train Acc: 0.4993, Val Loss: 0.6927, Val Acc: 0.5133\n","Model: RNNModel, Epoch 16/20, Train Loss: 0.6965, Train Acc: 0.4929, Val Loss: 0.6947, Val Acc: 0.4900\n","Model: RNNModel, Epoch 17/20, Train Loss: 0.6946, Train Acc: 0.4914, Val Loss: 0.6944, Val Acc: 0.4933\n","Model: RNNModel, Epoch 18/20, Train Loss: 0.6956, Train Acc: 0.4836, Val Loss: 0.6946, Val Acc: 0.4900\n","Model: RNNModel, Epoch 19/20, Train Loss: 0.6976, Train Acc: 0.4943, Val Loss: 0.6927, Val Acc: 0.4900\n"]}]}]}